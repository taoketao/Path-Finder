{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sun Oct  8 22:11:10 2017\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "# [adapted from an IPython log file]\n",
    "# This notebook hopes to perform a basic test on the newly-made\n",
    "# pathfinder environment.\n",
    "\n",
    "import time; print(time.asctime())\n",
    "'Fri Oct  6 00:58:37 2017'\n",
    "\n",
    "import sys, imp\n",
    "#sys.path.insert(0, '/home/usrnet/Software/installers/gym/')\n",
    "sys.path.insert(0, '/home/usrnet/Work-Files/Path-Finder')\n",
    "#sys.path.insert(0, '/home/usrnet/miniconda3/lib/python3.6/site-packages')\n",
    "sys.path.insert(0, '/home/usrnet/Software/installers/gym/examples/agents')\n",
    "\n",
    "import gym\n",
    "from gym import envs, spaces\n",
    "from gym.spaces import Discrete\n",
    "from random_agent import *\n",
    "from cem import *\n",
    "\n",
    "import tensorflow as tf\n",
    "import rl, keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Flatten\n",
    "from keras.optimizers import Adam\n",
    "from rl.agents.dqn import DQNAgent\n",
    "from rl.policy import EpsGreedyQPolicy\n",
    "from rl.policy import BoltzmannQPolicy\n",
    "from rl.memory import SequentialMemory\n",
    "\n",
    "import cleaner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "try: reload(cleaner)\n",
    "except: \n",
    "    import imp, cleaner; imp.reload(cleaner)\n",
    "#from cleaner import *\n",
    "import cleaner; from cleaner import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyEnv(gym.Env):\n",
    "    def __init__(self, exp_env):\n",
    "        self.action_space = Discrete(4)\n",
    "        sz = exp_env.getGridSize()\n",
    "        self.observation_space = spaces.Tuple((Discrete(sz[0]), \\\n",
    "                                Discrete(sz[1]), Discrete(4)))\n",
    "        self.current_state = exp_env.get_random_starting_state()\\\n",
    "                                                    ['state']\n",
    "        self.exp_env = exp_env        \n",
    "        self.metadata = {'render.modes':['human','ansi','PRINT','NOPRINT']}\n",
    "        self.reward_range = (0,1)\n",
    "        \n",
    "    def _reset(self): \n",
    "        self.current_state = self.exp_env.get_random_starting_state()['state']\n",
    "        return self.current_state \n",
    "    \n",
    "    def _render(self, mode=None, close=None): \n",
    "        p= print_state(self.current_state, \\\n",
    "                           'condensed', 'string_ret')\n",
    "        if mode in ['human','PRINT']: \n",
    "            print(p)\n",
    "            return p\n",
    "        elif mode=='NOPRINT': \n",
    "            return p\n",
    "        else: raise Exception(mode, 'render mode not defined')\n",
    "\n",
    "    def sample(self): return random.choice(list(DVECS.keys()))\n",
    "\n",
    "    def _step(self, actn):\n",
    "        new_st, succ = self.exp_env.new_statem(\\\n",
    "            self.current_state, actn, valid_move_too=True)\n",
    "        goalReached = self.exp_env.get_agent_loc(new_st) == \\\n",
    "                      self.exp_env.get_goal_loc(new_st)\n",
    "        self.current_state = new_st\n",
    "        return new_st, int(goalReached), (not succ) or goalReached, {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "flatten_1 (Flatten)          (None, 484)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 32)                15520     \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 4)                 132       \n",
      "=================================================================\n",
      "Total params: 15,652\n",
      "Trainable params: 15,652\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "my_env = MyEnv(cleaner.ExpAPI('tse2007', 'egocentric'))\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Flatten(input_shape = (1,11,11,4) ))\n",
    "model.add(Dense(32))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dense(my_env.action_space.n, activation='linear'))\n",
    "print(model.summary())\n",
    "memory = SequentialMemory(limit=1000, window_length=1)\n",
    "policy = EpsGreedyQPolicy()\n",
    "dqn = DQNAgent(model=model, nb_actions = my_env.action_space.n, memory=memory, nb_steps_warmup=10, enable_dueling_network=True, dueling_type='avg', target_model_update=1e-3, policy=policy)\n",
    "dqn.compile(Adam(lr=1e-5), metrics=['mae'])\n",
    "\n",
    "model2 = Sequential()\n",
    "model2.add(Flatten(input_shape = (1,11,11,4) ))\n",
    "model2.add(Dense(32))\n",
    "model2.add(Activation('relu'))\n",
    "model2.add(Dense(32))\n",
    "model2.add(Activation('relu'))\n",
    "model2.add(Dense(my_env.action_space.n, activation='linear'))\n",
    "print(model2.summary())\n",
    "memory = SequentialMemory(limit=10000, window_length=1)\n",
    "policy = BoltzmannQPolicy()\n",
    "dqn2 = DQNAgent(model=model2, nb_actions = my_env.action_space.n, memory=memory, nb_steps_warmup=300, enable_dueling_network=True, dueling_type='avg', target_model_update=1e-3, policy=policy)\n",
    "dqn2.compile(Adam(lr=1e-2), metrics=['mae'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for 16000 steps ...\n",
      "Interval 1 (0 steps performed)\n",
      "2000/2000 [==============================] - 9s - reward: 0.0095     \n",
      "301 episodes - episode_reward: 0.063 [0.000, 1.000] - loss: 0.004 - mean_absolute_error: 0.045 - mean_q: 0.078\n",
      "\n",
      "Interval 2 (2000 steps performed)\n",
      "2000/2000 [==============================] - 9s - reward: 0.0080     \n",
      "170 episodes - episode_reward: 0.094 [0.000, 1.000] - loss: 0.003 - mean_absolute_error: 0.058 - mean_q: 0.099\n",
      "\n",
      "Interval 3 (4000 steps performed)\n",
      "2000/2000 [==============================] - 9s - reward: 0.0070     \n",
      "106 episodes - episode_reward: 0.132 [0.000, 1.000] - loss: 0.003 - mean_absolute_error: 0.055 - mean_q: 0.097\n",
      "\n",
      "Interval 4 (6000 steps performed)\n",
      "2000/2000 [==============================] - 9s - reward: 0.0090     \n",
      "97 episodes - episode_reward: 0.186 [0.000, 1.000] - loss: 0.004 - mean_absolute_error: 0.079 - mean_q: 0.141\n",
      "\n",
      "Interval 5 (8000 steps performed)\n",
      "2000/2000 [==============================] - 9s - reward: 0.0055     \n",
      "83 episodes - episode_reward: 0.133 [0.000, 1.000] - loss: 0.003 - mean_absolute_error: 0.098 - mean_q: 0.174\n",
      "\n",
      "Interval 6 (10000 steps performed)\n",
      "2000/2000 [==============================] - 9s - reward: 0.0055     \n",
      "82 episodes - episode_reward: 0.134 [0.000, 1.000] - loss: 0.003 - mean_absolute_error: 0.097 - mean_q: 0.177\n",
      "\n",
      "Interval 7 (12000 steps performed)\n",
      "2000/2000 [==============================] - 9s - reward: 0.0040     \n",
      "52 episodes - episode_reward: 0.154 [0.000, 1.000] - loss: 0.002 - mean_absolute_error: 0.091 - mean_q: 0.176\n",
      "\n",
      "Interval 8 (14000 steps performed)\n",
      "2000/2000 [==============================] - 9s - reward: 0.0090     \n",
      "done, took 73.988 seconds\n",
      "Training for 16000 steps ...\n",
      "Interval 1 (0 steps performed)\n",
      "2000/2000 [==============================] - 12s - reward: 0.0100    \n",
      "738 episodes - episode_reward: 0.027 [0.000, 1.000] - loss: 0.004 - mean_absolute_error: 0.063 - mean_q: 0.086\n",
      "\n",
      "Interval 2 (2000 steps performed)\n",
      "2000/2000 [==============================] - 12s - reward: 0.0030    - ETA: 0s - reward: 0\n",
      "623 episodes - episode_reward: 0.010 [0.000, 1.000] - loss: 0.003 - mean_absolute_error: 0.064 - mean_q: 0.086\n",
      "\n",
      "Interval 3 (4000 steps performed)\n",
      "2000/2000 [==============================] - 12s - reward: 0.0065    \n",
      "599 episodes - episode_reward: 0.022 [0.000, 1.000] - loss: 0.003 - mean_absolute_error: 0.057 - mean_q: 0.080\n",
      "\n",
      "Interval 4 (6000 steps performed)\n",
      "2000/2000 [==============================] - 12s - reward: 0.0075    \n",
      "657 episodes - episode_reward: 0.023 [0.000, 1.000] - loss: 0.003 - mean_absolute_error: 0.065 - mean_q: 0.090\n",
      "\n",
      "Interval 5 (8000 steps performed)\n",
      "2000/2000 [==============================] - 12s - reward: 0.0075    \n",
      "703 episodes - episode_reward: 0.021 [0.000, 1.000] - loss: 0.003 - mean_absolute_error: 0.066 - mean_q: 0.090\n",
      "\n",
      "Interval 6 (10000 steps performed)\n",
      "2000/2000 [==============================] - 12s - reward: 0.0070    \n",
      "618 episodes - episode_reward: 0.023 [0.000, 1.000] - loss: 0.004 - mean_absolute_error: 0.067 - mean_q: 0.088\n",
      "\n",
      "Interval 7 (12000 steps performed)\n",
      "2000/2000 [==============================] - 12s - reward: 0.0045    \n",
      "667 episodes - episode_reward: 0.013 [0.000, 1.000] - loss: 0.003 - mean_absolute_error: 0.064 - mean_q: 0.091\n",
      "\n",
      "Interval 8 (14000 steps performed)\n",
      "2000/2000 [==============================] - 11s - reward: 0.0050    \n",
      "done, took 97.338 seconds\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f338c929b00>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dqn.fit(my_env, nb_steps=16000, log_interval=2000, visualize=False, verbose=1);\n",
    "dqn2.fit(my_env, nb_steps=16000, log_interval=2000, visualize=False, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above was some tests. Now, attempt to build a model as similar to original pathfinder as possible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----\n",
      "--  *\n",
      "--I  \n",
      "--   \n",
      "-----\n",
      "\n",
      "-----\n",
      "- * -\n",
      "-I  -\n",
      "-   -\n",
      "-----\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ruru_env_ego = MyEnv(cleaner.ExpAPI('r-u-ru', 'egocentric'))\n",
    "ruru_env_allo = MyEnv(cleaner.ExpAPI('r-u-ru', 'allocentric'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ego egocentric egocentric\n",
      "-----\n",
      "-----\n",
      "- I -\n",
      "-  *-\n",
      "-   -\n",
      "\n",
      "allo allocentric allocentric\n",
      "-----\n",
      "- I -\n",
      "-  *-\n",
      "-   -\n",
      "-----\n",
      "\n",
      "ego egocentric egocentric\n",
      "- * -\n",
      "-   -\n",
      "- I -\n",
      "-----\n",
      "-----\n",
      "\n",
      "allo allocentric allocentric\n",
      "-----\n",
      "-   -\n",
      "-  *-\n",
      "- I -\n",
      "-----\n",
      "\n",
      "ego egocentric egocentric\n",
      "-----\n",
      "  *--\n",
      "  I--\n",
      "   --\n",
      "-----\n",
      "\n",
      "allo allocentric allocentric\n",
      "-----\n",
      "- * -\n",
      "-  I-\n",
      "-   -\n",
      "-----\n",
      "\n",
      "ego egocentric egocentric\n",
      "-----\n",
      "-- * \n",
      "--I  \n",
      "--   \n",
      "-----\n",
      "\n",
      "allo allocentric allocentric\n",
      "-----\n",
      "- * -\n",
      "-I  -\n",
      "-   -\n",
      "-----\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in cardinals: # NSEW cardinals const global list of actions, inherited from environment.py\n",
    "    ruru_env_ego.reset()\n",
    "    ruru_env_ego.step(i)\n",
    "    _=ruru_env_ego.render(mode='PRINT')\n",
    "\n",
    "    ruru_env_allo.reset()\n",
    "    ruru_env_allo.step(i)\n",
    "    _=ruru_env_allo.render(mode='PRINT')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
